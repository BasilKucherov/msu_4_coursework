{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "from transforms import *\n",
    "from loss_functions import *\n",
    "from datasets import *\n",
    "from models import *\n",
    "from torchvision.transforms import Compose\n",
    "from clustering_metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.device'> cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "use_gpu = False\n",
    "if torch.cuda.is_available():\n",
    "    use_gpu = True\n",
    "    device = torch.device('cuda', 0)\n",
    "    \n",
    "print(type(device), device)\n",
    "\n",
    "# may benefit if network size/input/output is stable\n",
    "if use_gpu:\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_description):\n",
    "        if 'name' not in model_description:\n",
    "                return '[ERROR]: corrupted model description'\n",
    "\n",
    "        if model_description['name'] == 'DSCNN':\n",
    "                n_mels = model_description['n_mels']\n",
    "                in_shape = (n_mels, 32)\n",
    "                in_channels = model_description['in_channels']\n",
    "                ds_cnn_number = model_description['ds_cnn_number']\n",
    "                ds_cnn_size = model_description['ds_cnn_size']\n",
    "                is_classifier = model_description['is_classifier']\n",
    "                classes_number = 0 if not is_classifier else model_description['classes_number']\n",
    "\n",
    "                return DSCNN(in_channels, in_shape, ds_cnn_number, ds_cnn_size, is_classifier, classes_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierClosestKnown():\n",
    "    def __init__(self, embedding_block, embedding_size, device):\n",
    "        self.device = device\n",
    "        self.embedding_block = embedding_block\n",
    "        self.embedding_size = embedding_size\n",
    "        self.freeze(self.embedding_block)\n",
    "\n",
    "        self.class_ids = torch.empty((0, ), dtype=torch.int32)\n",
    "        self.class_names = []\n",
    "        self.class_embeddings = torch.empty((0, self.embedding_size), dtype=torch.float32).to(self.device)\n",
    "\n",
    "        self.current_id = 0\n",
    "    \n",
    "    def freeze(self, block) -> None:\n",
    "        for p in block.parameters():\n",
    "            p.requires_grad = False\n",
    "        block.eval()\n",
    "\n",
    "    def add_class(self, input, label):\n",
    "        embed = self.embedding_block(input.to(self.device))\n",
    "        mean_embed = torch.mean(embed, dim=0).unsqueeze(0)\n",
    "\n",
    "        self.class_embeddings = torch.cat((self.class_embeddings, mean_embed), dim=0)\n",
    "        self.class_names.append(label)\n",
    "        self.class_ids = torch.cat((self.class_ids, torch.tensor([self.current_id], dtype=torch.int32)))\n",
    "        self.current_id += 1\n",
    "    \n",
    "    def get_classes(self):\n",
    "        return self.class_names\n",
    "\n",
    "    def classify(self, inputs):\n",
    "        input_embed = self.embedding_block(inputs.to(self.device))\n",
    "        distances = torch.cdist(input_embed, self.class_embeddings)  # Compute distances between input embeddings and class embeddings\n",
    "\n",
    "        closest_ids = torch.argmin(distances, dim=1).cpu().tolist()  # Find the index of the closest known embedding for each input\n",
    "        labels = [self.class_names[id] for id in closest_ids]  # Get the corresponding class labels\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_class_batch(dataset, class_indices, samples_number, class_idx):\n",
    "    indexes = np.random.choice(class_indices[class_idx], samples_number, replace=False)\n",
    "\n",
    "    batch = []\n",
    "\n",
    "    for i in indexes:\n",
    "        item = dataset.__getitem__(i)\n",
    "        batch.append(item['input'])\n",
    "    \n",
    "    batch = torch.stack(batch, dim=0)\n",
    "    return batch\n",
    "\n",
    "def eval_experiment_closest_known(experiments_folder, experiment_name, epochs_to_test=[], fsl_examples=5, random_seed=42):\n",
    "    experiment_folder = os.path.join(experiments_folder, experiment_name)\n",
    "\n",
    "    experiment_settings_path = os.path.join(experiment_folder, \"experiment_settings.json\")\n",
    "    experiment_stats_path = os.path.join(experiment_folder, \"stats.json\")\n",
    "\n",
    "    experiment_stats = {}\n",
    "    if os.path.isfile(experiment_stats_path):\n",
    "        with open(experiment_stats_path, 'r') as fp:\n",
    "            experiment_stats = json.load(fp)\n",
    "\n",
    "    with open(experiment_settings_path, 'r') as fp:\n",
    "        experiment_settings = json.load(fp)\n",
    "\n",
    "\n",
    "    ''' create model '''\n",
    "    experiment_settings['model']['is_classifier'] = False\n",
    "    model = create_model(experiment_settings['model'])\n",
    "\n",
    "    n_mels = experiment_settings['model']['n_mels']\n",
    "    embedding_size = experiment_settings['model']['ds_cnn_size']\n",
    "\n",
    "    ''' prepare datasets '''\n",
    "    test_dataset_path = 'datasets/speech_commands/test'\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    feature_transform = Compose([ToSTFT(), ToMelSpectrogramFromSTFT(n_mels=n_mels), ToTensor('mel_spectrogram', 'input')])\n",
    "\n",
    "\n",
    "    test_dataset = SpeechCommandsDataset(test_dataset_path,\n",
    "                                    Compose([LoadAudio(),\n",
    "                                            FixAudioLength(),\n",
    "                                            feature_transform]))\n",
    "\n",
    "    dl_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=16, prefetch_factor=2)\n",
    "    test_indices = test_dataset.get_class_indices()\n",
    "    test_classes = test_dataset.classes\n",
    "\n",
    "    ''' Prepare train data '''\n",
    "    np.random.seed(random_seed)\n",
    "    n_samples = fsl_examples\n",
    "    classifier_train_batches = []\n",
    "    classifier_train_labels = []\n",
    "        \n",
    "    for class_name in test_classes:\n",
    "        class_idx = test_dataset.get_idx_from_class(class_name)\n",
    "        classifier_train_batches.append(form_class_batch(test_dataset, test_indices, n_samples, class_idx))\n",
    "        classifier_train_labels.append(class_name)\n",
    "\n",
    "    all_class_names = test_classes\n",
    "    class_name_to_idx = {class_name: i for i, class_name in enumerate(all_class_names)} \n",
    "\n",
    "    ''' get epochs to test '''\n",
    "    if not epochs_to_test:\n",
    "        for key in experiment_stats['clustering_metrics']:\n",
    "            epochs_to_test.append(experiment_stats['clustering_metrics'][key]['best_train_epoch'])\n",
    "            epochs_to_test.append(experiment_stats['clustering_metrics'][key]['best_valid_epoch'])\n",
    "            epochs_to_test.append(experiment_stats['clustering_metrics'][key]['best_test_epoch'])\n",
    "        \n",
    "        epochs_to_test.append(experiment_stats['loss']['best_train_epoch'])\n",
    "\n",
    "        if ('best_valid_epoch' in experiment_stats['loss'].keys()) and experiment_stats['loss']['best_valid_epoch']:\n",
    "            epochs_to_test.append(experiment_stats['loss']['best_valid_epoch'])\n",
    "\n",
    "    epochs_to_test = set(epochs_to_test)\n",
    "    print(f\"epochs to test = {epochs_to_test}\")\n",
    "\n",
    "    ''' actual testing '''\n",
    "    print(f\"Epochs to test: {epochs_to_test}\")\n",
    "\n",
    "    for epoch in epochs_to_test:\n",
    "        print(f\"{epoch} \", end=\"\")\n",
    "        checkpoint_fname = os.path.join(experiment_folder, 'checkpoints', f'checkpoint_{epoch}')\n",
    "\n",
    "        checkpoint = torch.load(checkpoint_fname)\n",
    "        state_dict = checkpoint['state_dict']\n",
    "        cur_epoch = checkpoint['epoch']\n",
    "        remove_prefix = 'module.'\n",
    "        state_dict = {k[len(remove_prefix):] if k.startswith(remove_prefix) else k: v for k, v in state_dict.items()}\n",
    "\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        model.freeze()\n",
    "\n",
    "        em_classifier = ClassifierClosestKnown(model, embedding_size, device)\n",
    "\n",
    "        ''' Actual Training of classifier'''\n",
    "        for i, batch in enumerate(classifier_train_batches):\n",
    "            batch = torch.unsqueeze(batch, 1)\n",
    "            em_classifier.add_class(batch, classifier_train_labels[i])\n",
    "\n",
    "        ''' test classifier on test'''\n",
    "        test_true_labels = []\n",
    "        test_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dl_test:\n",
    "                input = torch.unsqueeze(batch['input'].to(device), 1)\n",
    "\n",
    "                target = [test_dataset.get_class_from_idx(item.item()) for item in batch['target']]\n",
    "                target_idx = [class_name_to_idx[t] for t in target]\n",
    "\n",
    "                prediction = em_classifier.classify(input)\n",
    "                prediction_idx = [class_name_to_idx[p] for p in prediction]\n",
    "\n",
    "                test_predictions += prediction_idx\n",
    "                test_true_labels += target_idx\n",
    "        \n",
    "        test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "\n",
    "        fsl_key = f'closest_known_epoch{epoch}_shots{fsl_examples}'\n",
    "\n",
    "        if 'fsl_only_test' not in experiment_stats.keys():\n",
    "            experiment_stats['fsl_only_test'] = {}\n",
    "\n",
    "        if fsl_key in experiment_stats['fsl_only_test'].keys():\n",
    "            if not isinstance(experiment_stats['fsl_only_test'][fsl_key]['accuracy'], list):\n",
    "                experiment_stats['fsl_only_test'][fsl_key]['accuracy'] = [experiment_stats['fsl_only_test'][fsl_key]['accuracy']]\n",
    "            experiment_stats['fsl_only_test'][fsl_key]['accuracy'].append(test_accuracy)\n",
    "        else:\n",
    "            experiment_stats['fsl_only_test'][fsl_key] = {\n",
    "                \"epoch\": epoch,\n",
    "                \"shots\": fsl_examples,\n",
    "                \"accuracy\": test_accuracy\n",
    "            }\n",
    "    print()\n",
    "    with open(experiment_stats_path, \"w\") as fp:\n",
    "        json.dump(experiment_stats, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "experiments_folder = './experiments'\n",
    "\n",
    "items = os.listdir(experiments_folder)\n",
    "to_do_list = [item for item in items if os.path.isdir(os.path.join(experiments_folder, item))]\n",
    "to_do_list = sorted(to_do_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lifted_structured_test', 'triplet_br_test']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_to_list = []\n",
    "\n",
    "for exp_name in to_do_list:\n",
    "    if 'test' in exp_name:\n",
    "        new_to_list.append(exp_name)\n",
    "\n",
    "to_do_list = new_to_list\n",
    "to_do_list = ['lifted_structured_test', 'triplet_br_test']\n",
    "to_do_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----lifted_structured_test-----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510b5e557b064e178efc836d46b7a0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 121958:\n",
      "epochs to test = {198, 199, 40, 105, 185, 60, 190}\n",
      "Epochs to test: {198, 199, 40, 105, 185, 60, 190}\n",
      "198 199 40 105 185 60 190 \n",
      "seed = 671155:\n",
      "epochs to test = {198, 199, 40, 105, 185, 60, 190}\n",
      "Epochs to test: {198, 199, 40, 105, 185, 60, 190}\n",
      "198 199 40 105 185 60 190 \n",
      "seed = 131932:\n",
      "epochs to test = {198, 199, 40, 105, 185, 60, 190}\n",
      "Epochs to test: {198, 199, 40, 105, 185, 60, 190}\n",
      "198 199 40 105 185 60 190 \n",
      "seed = 365838:\n",
      "epochs to test = {198, 199, 40, 105, 185, 60, 190}\n",
      "Epochs to test: {198, 199, 40, 105, 185, 60, 190}\n",
      "198 199 40 105 185 60 190 \n",
      "seed = 259178:\n",
      "epochs to test = {198, 199, 40, 105, 185, 60, 190}\n",
      "Epochs to test: {198, 199, 40, 105, 185, 60, 190}\n",
      "198 199 40 105 185 60 190 \n",
      "seed = 644167:\n",
      "epochs to test = {198, 199, 40, 105, 185, 60, 190}\n",
      "Epochs to test: {198, 199, 40, 105, 185, 60, 190}\n",
      "198 199 40 105 185 60 190 \n",
      "seed = 110268:\n",
      "epochs to test = {198, 199, 40, 105, 185, 60, 190}\n",
      "Epochs to test: {198, 199, 40, 105, 185, 60, 190}\n",
      "198 199 40 105 185 60 190 \n",
      "seed = 732180:\n",
      "epochs to test = {198, 199, 40, 105, 185, 60, 190}\n",
      "Epochs to test: {198, 199, 40, 105, 185, 60, 190}\n",
      "198 199 40 105 185 60 190 \n",
      "seed = 54886:\n",
      "epochs to test = {198, 199, 40, 105, 185, 60, 190}\n",
      "Epochs to test: {198, 199, 40, 105, 185, 60, 190}\n",
      "198 199 40 105 185 60 190 \n",
      "seed = 137337:\n",
      "epochs to test = {198, 199, 40, 105, 185, 60, 190}\n",
      "Epochs to test: {198, 199, 40, 105, 185, 60, 190}\n",
      "198 199 40 105 185 60 190 \n",
      "-----triplet_br_test-----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ced98cf1044b5fbce78356550d2389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 121958:\n",
      "epochs to test = {35, 195, 175, 180, 155, 60, 190}\n",
      "Epochs to test: {35, 195, 175, 180, 155, 60, 190}\n",
      "35 195 175 180 155 60 190 \n",
      "seed = 671155:\n",
      "epochs to test = {35, 195, 175, 180, 155, 60, 190}\n",
      "Epochs to test: {35, 195, 175, 180, 155, 60, 190}\n",
      "35 195 175 180 155 60 190 \n",
      "seed = 131932:\n",
      "epochs to test = {35, 195, 175, 180, 155, 60, 190}\n",
      "Epochs to test: {35, 195, 175, 180, 155, 60, 190}\n",
      "35 195 175 180 155 60 190 \n",
      "seed = 365838:\n",
      "epochs to test = {35, 195, 175, 180, 155, 60, 190}\n",
      "Epochs to test: {35, 195, 175, 180, 155, 60, 190}\n",
      "35 195 175 180 155 60 190 \n",
      "seed = 259178:\n",
      "epochs to test = {35, 195, 175, 180, 155, 60, 190}\n",
      "Epochs to test: {35, 195, 175, 180, 155, 60, 190}\n",
      "35 195 175 180 155 60 190 \n",
      "seed = 644167:\n",
      "epochs to test = {35, 195, 175, 180, 155, 60, 190}\n",
      "Epochs to test: {35, 195, 175, 180, 155, 60, 190}\n",
      "35 195 175 180 155 60 190 \n",
      "seed = 110268:\n",
      "epochs to test = {35, 195, 175, 180, 155, 60, 190}\n",
      "Epochs to test: {35, 195, 175, 180, 155, 60, 190}\n",
      "35 195 175 180 155 60 190 \n",
      "seed = 732180:\n",
      "epochs to test = {35, 195, 175, 180, 155, 60, 190}\n",
      "Epochs to test: {35, 195, 175, 180, 155, 60, 190}\n",
      "35 195 175 180 155 60 190 \n",
      "seed = 54886:\n",
      "epochs to test = {35, 195, 175, 180, 155, 60, 190}\n",
      "Epochs to test: {35, 195, 175, 180, 155, 60, 190}\n",
      "35 195 175 180 155 60 190 \n",
      "seed = 137337:\n",
      "epochs to test = {35, 195, 175, 180, 155, 60, 190}\n",
      "Epochs to test: {35, 195, 175, 180, 155, 60, 190}\n",
      "35 195 175 180 155 60 190 \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "seeds = np.random.randint(0, 1000000, 10)\n",
    "\n",
    "\n",
    "for experiment_name in to_do_list:\n",
    "    print(f\"-----{experiment_name}-----\")\n",
    "    experiment_folder = os.path.join(experiments_folder, experiment_name)\n",
    "\n",
    "    for i in tqdm.tqdm(range(10)):\n",
    "        print(f\"seed = {seeds[i]}:\")\n",
    "        eval_experiment_closest_known(experiments_folder, experiment_name, epochs_to_test=[], fsl_examples=5, random_seed=seeds[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
